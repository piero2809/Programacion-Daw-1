#!/usr/bin/env python3
import os
import json
import torch
import platform
from datetime import datetime

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import (
    LoraConfig,
    get_peft_model,
)

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------

DATA_PATH = "outputs/*.jsonl"  # JSONL generated by your pretraining script

MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512
NUM_EPOCHS = 3
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4


# -------------------------------------------------------------------
# REPORTING
# -------------------------------------------------------------------

def generate_markdown_report(
    *,
    start_dt: datetime,
    end_dt: datetime,
    device: str,
    data_path_pattern: str,
    jsonl_files,
    dataset_size: int,
    training_args: TrainingArguments,
    train_metrics: dict | None,
) -> str:
    """
    Genera un informe Markdown en OUTPUT_DIR con trazabilidad del entrenamiento:
    - tiempos
    - modelo, dispositivo y par√°metros
    - informaci√≥n del sistema
    - lista de ficheros JSONL usados y n√∫mero de ejemplos por fichero
    - m√©tricas devueltas por Trainer.train()
    """
    duration = end_dt - start_dt
    epoch = int(end_dt.timestamp())
    timestamp_str = end_dt.strftime("%Y%m%d_%H%M%S")
    report_name = f"reporte_entrenamiento_{timestamp_str}_{epoch}.md"
    report_path = os.path.join(OUTPUT_DIR, report_name)

    try:
        import transformers
    except ImportError:
        transformers = None

    try:
        import peft as peft_mod
    except ImportError:
        peft_mod = None

    # Informaci√≥n del sistema / entorno
    system_info = {
        "Sistema operativo": platform.system(),
        "Versi√≥n del sistema": platform.version(),
        "Plataforma": platform.platform(),
        "M√°quina": platform.machine(),
        "Procesador": platform.processor(),
        "Python": platform.python_version(),
        "PyTorch": torch.__version__,
        "Transformers": getattr(transformers, "__version__", "desconocido") if transformers else "no instalado",
        "PEFT": getattr(peft_mod, "__version__", "desconocido") if peft_mod else "no instalado",
        "CUDA disponible": torch.cuda.is_available(),
        "Dispositivo de entrenamiento": device,
        "Directorio de trabajo": os.getcwd(),
    }

    # Estad√≠sticas por fichero JSONL (n√∫mero de l√≠neas no vac√≠as)
    per_file_counts = []
    total_lines = 0
    for path in sorted(jsonl_files):
        count = 0
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        count += 1
        except Exception as e:
            per_file_counts.append({
                "file": path,
                "count": None,
                "error": str(e),
            })
            continue
        per_file_counts.append({
            "file": path,
            "count": count,
            "error": None,
        })
        total_lines += count

    # Construcci√≥n del Markdown
    lines: list[str] = []

    # Cabecera
    lines.append("# Informe de entrenamiento del modelo Qwen2.5\n")
    lines.append("## Resumen de la ejecuci√≥n\n")
    lines.append(f"- **Fecha/hora de inicio:** {start_dt.strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"- **Fecha/hora de finalizaci√≥n:** {end_dt.strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"- **Duraci√≥n total:** {duration}")
    lines.append(f"- **Modelo base:** `{MODEL_NAME}`")
    lines.append(f"- **Directorio de salida del modelo:** `{OUTPUT_DIR}`")
    lines.append(f"- **Patr√≥n de datos utilizado:** `{data_path_pattern}`")
    lines.append(f"- **N√∫mero total de ejemplos en el dataset (seg√∫n datasets):** {dataset_size}")
    lines.append(f"- **N√∫mero total de l√≠neas no vac√≠as en JSONL (suma de ficheros):** {total_lines}")
    lines.append(f"- **N√∫mero de ficheros JSONL:** {len(jsonl_files)}")
    lines.append(f"- **Dispositivo usado:** {device}")
    lines.append("")

    # Par√°metros de entrenamiento
    lines.append("## Par√°metros de entrenamiento\n")
    lines.append(f"- **NUM_EPOCHS:** {NUM_EPOCHS}")
    lines.append(f"- **LR (learning_rate):** {LR}")
    lines.append(f"- **BATCH_SIZE (per_device_train_batch_size):** {BATCH_SIZE}")
    lines.append(f"- **GRAD_ACCUM (gradient_accumulation_steps):** {GRAD_ACCUM}")
    lines.append(f"- **MAX_LENGTH:** {MAX_LENGTH}")
    lines.append("")
    lines.append("### TrainingArguments efectivos\n")
    for k, v in sorted(training_args.to_dict().items()):
        lines.append(f"- **{k}:** {v}")
    lines.append("")

    # M√©tricas de entrenamiento
    lines.append("## M√©tricas de entrenamiento\n")
    if train_metrics:
        for k, v in train_metrics.items():
            lines.append(f"- **{k}:** {v}")
    else:
        lines.append("- No se recibieron m√©tricas desde `Trainer.train()`.")
    lines.append("")

    # Informaci√≥n del sistema
    lines.append("## Informaci√≥n del sistema y entorno\n")
    for k, v in system_info.items():
        lines.append(f"- **{k}:** {v}")
    lines.append("")

    # √çndice de ficheros JSONL
    lines.append("## √çndice de ficheros JSONL utilizados\n")
    for idx, info in enumerate(per_file_counts, start=1):
        base = os.path.basename(info["file"])
        lines.append(f"- [Archivo {idx}: `{base}`](#archivo-jsonl-{idx})")
    lines.append("")

    # Detalle por fichero JSONL
    for idx, info in enumerate(per_file_counts, start=1):
        lines.append(f"<a name=\"archivo-jsonl-{idx}\"></a>")
        lines.append(f"### Archivo JSONL {idx}\n")
        lines.append(f"- **Ruta:** `{info['file']}`")
        if info["error"] is not None:
            lines.append(f"- **Error al leer:** `{info['error']}`")
        else:
            lines.append(f"- **N√∫mero de l√≠neas (ejemplos Q/A):** {info['count']}")
        lines.append("")

    # Escritura a disco
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"üìù Informe de entrenamiento generado en: {report_path}")
    return report_path


# -------------------------------------------------------------------
# MAIN
# -------------------------------------------------------------------

def main():
    start_dt = datetime.now()

    print("üöÄ Starting Qwen2.5 training (Q/A JSONL)")
    print(f"üìÅ Dataset(s): {DATA_PATH}")
    print(f"üß† Base model: {MODEL_NAME}")
    print("-" * 60)

    # -------------------------------------------------------------------
    # Ensure outputs/ has JSONL files
    # -------------------------------------------------------------------
    base_dir = os.path.dirname(DATA_PATH.split("*", 1)[0]) or "."
    if not os.path.isdir(base_dir):
        raise FileNotFoundError(f"Data directory not found: {base_dir}")

    jsonl_files = [
        os.path.join(base_dir, f)
        for f in os.listdir(base_dir)
        if f.endswith(".jsonl")
    ]
    if not jsonl_files:
        raise FileNotFoundError(
            f"No .jsonl files found in {base_dir}. "
            "Run the pretraining script first."
        )

    # -------------------------------------------------------------------
    # Device
    # -------------------------------------------------------------------
    if torch.cuda.is_available():
        device = "cuda"
        print("üíª CUDA GPU detected. Training in fp16 with LoRA (no 4-bit).")
    else:
        device = "cpu"
        print("üíª No CUDA GPU. Training on CPU (this will be slower).")

    # -------------------------------------------------------------------
    # Load dataset
    # -------------------------------------------------------------------
    print("üì• Loading dataset with datasets.load_dataset(...)")
    raw_dataset = load_dataset(
        "json",
        data_files=DATA_PATH,
        split="train"
    )
    print(f"‚úÖ Dataset loaded with {len(raw_dataset)} Q/A examples.")

    # -------------------------------------------------------------------
    # Tokenizer and model (Qwen)
    # -------------------------------------------------------------------
    print("‚úÖ Loading tokenizer (Qwen)...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("‚úÖ Loading base model (Qwen)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model.to(device)

    # -------------------------------------------------------------------
    # LoRA in full precision (no bitsandbytes)
    # -------------------------------------------------------------------
    print("‚úÖ Wrapping model with LoRA (full precision)...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # -------------------------------------------------------------------
    # Convert question/answer ‚Üí chat text
    # -------------------------------------------------------------------
    print("üß± Converting (question/answer) pairs into chat-style text...")

    SYSTEM_PROMPT = (
        "Eres un asistente educativo en espa√±ol que responde de forma clara, "
        "precisa y concisa a preguntas t√©cnicas."
    )

    def qa_to_text(example):
        q = example.get("question", "")
        a = example.get("answer", "")
        if not isinstance(q, str):
            q = str(q)
        if not isinstance(a, str):
            a = str(a)

        conv = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": q},
            {"role": "assistant", "content": a},
        ]

        try:
            text = tokenizer.apply_chat_template(
                conv,
                tokenize=False,
                add_generation_prompt=False,
            )
        except Exception:
            # Fallback text if apply_chat_template is missing or fails
            parts = []
            for m in conv:
                if m["role"] == "user":
                    prefix = "Usuario"
                elif m["role"] == "assistant":
                    prefix = "Asistente"
                else:
                    prefix = m["role"]
                parts.append(f"{prefix}: {m['content']}")
            text = "\n".join(parts)

        return {"text": text}

    text_dataset = raw_dataset.map(
        qa_to_text,
        remove_columns=[c for c in raw_dataset.column_names
                        if c not in ("question", "answer")]
    )

    # -------------------------------------------------------------------
    # Tokenization
    # -------------------------------------------------------------------
    print("‚úÖ Tokenizing dataset...")

    def tokenize_fn(batch):
        out = tokenizer(
            batch["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length",
        )
        out["labels"] = out["input_ids"].copy()
        return out

    tokenized_dataset = text_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text", "question", "answer"]
    )

    # -------------------------------------------------------------------
    # TrainingArguments
    # -------------------------------------------------------------------
    print("‚úÖ Configuring TrainingArguments...")

    use_fp16 = (device == "cuda")
    use_bf16 = False

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        weight_decay=0.01,
        warmup_ratio=0.03,
        logging_steps=10,
        save_steps=200,
        save_total_limit=1,
        fp16=use_fp16,
        bf16=use_bf16,
        dataloader_pin_memory=False,
        report_to="none",
    )

    # -------------------------------------------------------------------
    # Trainer
    # -------------------------------------------------------------------
    print("‚úÖ Creating Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    # -------------------------------------------------------------------
    # Train
    # -------------------------------------------------------------------
    print("üöÇ Starting training...")
    train_output = trainer.train()
    print("üèÅ Training finished.")

    # -------------------------------------------------------------------
    # Save
    # -------------------------------------------------------------------
    print("üíæ Saving model and tokenizer to", OUTPUT_DIR)
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("‚úÖ Done.")

    # -------------------------------------------------------------------
    # Report
    # -------------------------------------------------------------------
    end_dt = datetime.now()
    train_metrics = getattr(train_output, "metrics", None) if train_output is not None else None

    generate_markdown_report(
        start_dt=start_dt,
        end_dt=end_dt,
        device=device,
        data_path_pattern=DATA_PATH,
        jsonl_files=jsonl_files,
        dataset_size=len(raw_dataset),
        training_args=training_args,
        train_metrics=train_metrics,
    )


if __name__ == "__main__":
    main()

